\section{Algorithms Explanation} \label{section:algorithms}

To see the effects of cache locality and parallel computing on matrix multiplication implementations, we used five different matrix multiplication algorithms, described in this section. These algorithms perform the same operations, ending in the same results, only differing by the order of the operations done, which, as we will see, will have crucial impacts on each algorithm's performance.

\subsection{Classic Matrix Multiplication}

The \textit{classic} matrix multiplication algorithm is closely related to its hand calculation. This method for multiplication of two matrices: $A \times B = C$, where $A(m, n)$, $B(n, p)$ and $C(m, p)$, obtains the sum of a cell $(m,p)$ in the solution, by linear combination of row $m$ of A and column $p$ of C.

Implementing this algorithm is fairly straighforward, as we only need to iterate through every row and column of the solution matrix, and, for each cell, perform the linear combination of the values:

\begin{figure}[h!]
\begin{verbatim}
for (i = 0; i < m; i++)
    for (j = 0; j < p; j++)
        for (k = 0; k < n; k++)
\end{verbatim}
\caption{Overview of Classic Matrix Multiplication Algorithm}
\label{fig:algo:classic}
\end{figure}

As we can expect from the 3 nested loops, the overall time complexity for the matrix multiplication algorithm is $Î˜(n^3)$. This time complexity is maintained among all the implemented algorithms, therefore performance impacts are mainle caused by cache accesses and parallelism.
    
\subsection{Line Matrix Multiplication}

The line matrix multiplication algorithm is very similar to the previous algorithm. The only difference is that the second and third loops switch places:

\begin{figure}[h!]
\begin{verbatim}
for (i = 0; i < m; i++)
    for (k = 0; k < n; k++)
        for (j = 0; j < p; j++)
\end{verbatim}
\caption{Overview of Line Matrix Multiplication Algorithm}
\label{fig:algo:line}
\end{figure}

In practice, the difference between this and the previous approach is that instead of calculating the result matrix cell by cell, we instead focus on calculating and storing some terms of the linear combination line by line.

Although the correctness and overall time complexity of this algorithm remains the same, due to the way the matrices are stored and the memory is accesed, this allows for fewer cache misses, leading to performance improvements.

\subsection{Block Matrix Multiplication}

The last single-core algorithm implemented is the block matrix multiplication algorithm. This approach expands on top of the previous one and the same logic applied in calculating subresults line by line is now used to calculate the solution block by block. For this, we need 6 nested loops:

\begin{figure}[h!]
\begin{verbatim}
for (I = 0; I < m; I += block_size)
    for (K = 0; K < n; K += block_size)
        for (J = 0; J < p; J += block_size)
            for (i = I; i < min(I + block_size, m); i++)
                for (k = K; k < min(K + block_size, n); k++)
                    for (j = J; j < min(J + block_size, p); j++)
\end{verbatim}
\caption{Overview of Block Matrix Multiplication Algorithm}
\label{fig:algo:block}
\end{figure}

The main idea is that now, we have divided the matrix into smaller blocks and the operations are performed block by block. Again, due to the way the memory is accessed, this leads to better performance and less cache misses because the local block information is already stored in cache.

\subsection{Parallel Matrix Multiplication}

Besides the previous three single-core algorithms, we also implemented two variations of the line algorithm using multi-core processing. For this, we used the OpenMP API for parallel programming in C++. The next sections show an explanation and overview of the implementations.

\subsubsection{First Version}

The first approach aims to parallelize the outermost loop (i) of the line algorithm. There will be a division of i values between the different threads so that each one works on a separate row at the same time (they also get their own k and j variables). Due to working on separate rows, the threads do not need synchronization.

For this, we used OpenMP's \verb#parallel for# and \verb#private# variable pragmas:

\begin{figure}[h!]
\begin{verbatim}
#pragma omp parallel for private(i, k, j)
for (i = 0; i < m; i++)
    for (k = 0; k < n; k++)
        for (j = 0; j < p; j++)
\end{verbatim}
\caption{Overview of First Parallel Matrix Multiplication Algorithm}
\label{fig:algo:parallel1}
\end{figure}

\subsubsection{Second Version}

The second approach aims to parallelize the innemost loop (j) of the line algorithm. The thread pool will work on the same row at the same time but on different columns. In this case, the threads need to be synchronized after finishing the row operations which can lead to some overhead.

For this, we used OpenMP's \verb#parallel# region, \verb#for# and \verb#private# variable pragmas:

\begin{figure}[h!]
\begin{verbatim}
#pragma omp parallel private(i, k)
for (i = 0; i < m; i++)
    for (k = 0; k < n; k++)
        #pragma omp for private(j)
        for (j = 0; j < p; j++)
\end{verbatim}
\caption{Overview of Second Parallel Matrix Multiplication Algorithm}
\label{fig:algo:parallel2}
\end{figure}
